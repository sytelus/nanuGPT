apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  generateName: ${JOB_NAME}
  namespace: ${VOLCANO_NAMESPACE}
  labels:
    submitter: "${USER_ALIAS}"
spec:
  queue: ${VOLCANO_NAMESPACE}
  minAvailable: ${NODES} # this is number of pods in k8s, one pod=one node in volcano
  plugins:
    ssh: []        # passwordless SSH + /etc/volcano hostfiles
    svc: []        # headless Services when containerPorts exist
    env: []        # VC_* envs (host lists, etc.)
    pytorch:    # sets MASTER_ADDR/PORT, WORLD_SIZE, RANK for DDP
      - --master=master
      - --worker=worker
      - --port=${CONTAINER_PORT}
  # policies:
  #   - event: PodFailed
  #     action: RestartJob
  tasks:
    - name: master
      replicas: 1 # should be one for one job
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        metadata:
          labels:
            app: ${JOB_NAME}
            role: master
        spec: &podspec
          schedulerName: volcano
          restartPolicy: Never
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: ${MEMORY_SIZE_LIMIT}
            - name: data
              persistentVolumeClaim:
               claimName: ${VOLCANO_DATA_PVC_NAME}
          tolerations:
            - key: "rdma"
              operator: "Exists"
              effect: "NoSchedule"
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      app: ${JOB_NAME}
                  topologyKey: kubernetes.io/hostname
          containers:
            - name: master
              image: ${CONTAINER_IMAGE_PATH}
              imagePullPolicy: IfNotPresent
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
                - name: data
                  mountPath: /data
              ports:
                - name: torch
                  containerPort: ${CONTAINER_PORT}   # pytorch MASTER_PORT; svc plugin creates DNS
              env:
                - name: NPROC_PER_NODE
                  value: "${GPUS_PER_NODE}"             # spawn one worker per GPU on each node
                - name: GPUS_PER_NODE
                  value: "${GPUS_PER_NODE}"
                - name: NODES
                  value: "${NODES}"
                - name: OMP_NUM_THREADS
                  value: "${OMP_NUM_THREADS}"
                - name: NCCL_DEBUG
                  value: "${NCCL_DEBUG}"
                - name: NCCL_IB_DISABLE
                  value: "${NCCL_IB_DISABLE}"
                - name: PYTHONUNBUFFERED
                  value: "${PYTHONUNBUFFERED}"

              command: ["/bin/bash","-lc"]
              args:
                - |
                  # set -eu -o pipefail -o xtrace # fail if any command failes, log all commands, -o xtrace
                  # nvidia-smi

                  # this will setup other env vars in container
                  ${ENV_VARS}

                  # move to user-specific devbox directory on volume mounted as data
                  mkdir -p /data/${USER_ALIAS}/devbox
                  cd /data/${USER_ALIAS}/devbox

                  sleep infinity

              resources:
                requests: &requests
                  nvidia.com/gpu: "${GPUS_PER_NODE}"
                  cpu: "${CPU_REQUESTS}"
                  memory: "${MEMORY_REQUESTS}"
                  rdma/rdma_shared_device_a: "${RDMA_REQUESTS}"
                limits: *requests

    - name: worker
      replicas: ${WORKERS} # number of worker nodes = total nodes - 1 master node
  # policies removed: using action: None is invalid per Volcano validation webhook.
  # We don't want worker TaskCompleted to trigger CompleteJob (only master does that),
  # so we simply omit a policy for TaskCompleted here.
      template:
        metadata:
          labels:
            app: ${JOB_NAME}
            role: worker
        # Reuse the exact same pod spec (containers, resources, env, etc.)
        spec: *podspec
        # If you need worker-specific tweaks later, switch to:
        # spec:
        #   <<: *podspec
        #   containers:
        #     - <<: *trainer
        #       # add/override worker-only fields here