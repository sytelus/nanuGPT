# base config is designed for GPT2/OpenWebText with NanoGPT defaults
general:
  project_name: 'arithmatic'
  run_name: '_time:' # default run name is timestamp
  run_description: null
  out_dir: '$OUT_DIR/$project_name/$run_name'
  device_type: 'cuda'            # auto select if blank or cpu, cuda
  distributed_backend: 'nccl'       # nccl, gloo, mpi, horovod)
  distributed_init_method: 'env://' # for horovod, use 'tcp://localhost:23456'
  torch_compile: true # will not compile if Python > 3.11, torch < 2.1.0 or Windows
  seed: 42
  dtype: 'bfloat16'         # float32, float16, bfloat16
  enable_distributed: null # automatically set to true if world_size > 1

_env: # creates env vars
  project_name: '_copy: /general/project_name'
  run_name: '_copy: /general/run_name'

logging:
  project_name: '_copy: /general/project_name'
  run_name: '_copy: /general/run_name'
  log_dir: '_copy: /general/out_dir'
  run_description: '_copy: /general/run_description'
  enable_wandb: true
  log_filename: 'log.txt'
  summaries_filename: 'summaries.txt'
  allow_overwrite_log: true
  metrics_type: 'classification'
  summaries_stdout: true

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 12
    n_embd: 768
    n_head: 12
    context_length: 1024

scaler:
  module: 'nanugpt.scalers.amp_grad_scaler.get_scaler'
  module_kwargs: {}

tokenizer:
  module: 'nanugpt.tokenizers.arithmatic_tokenizer.get_tokenizer_factory'
  module_kwargs: {}

loss:
  module: 'nanugpt.losses.arithmatic_loss.get_loss_factory'
  module_kwargs:
    eq_token_id: 13
    pad_token_id: 11
    eos_token_id: 12

data:
  module: 'nanugpt.data.arithmatic_data.get_data'
  module_kwargs:
    min_digits: 1
    max_digits: 5
    max_samples: 10000
    device_batch_size: '_copy: /training/device_batch_size'
    eval_batch_size: 64 # 2^15=32768
    data_loader_seed: 8
    context_length: '_copy: /model/module_kwargs/context_length'

training:
  device_batch_size: 64
  max_steps: 20000
  enable_train_log: true
  log_every: 20
  grad_clip: 1.0 # disabled if 0.0
  global_batch_size: 512

optimizer:
  module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
  module_kwargs:
    learning_rate: 6.0E-4 # default 6.0E-4, GH200 12.0E-4
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    eps: 1.0E-8 # pytorch default
    zero_stage: 0 # 0: off, 1: shard optimizer

scheduler:
  module: 'nanugpt.schedulers.linear.get_scheduler'
  module_kwargs:
    warmup_iters: 2000
    max_iters: '_copy: /training/max_steps'
    end_factor: 1.0E-1

generate:
  checkpoint_path: '$OUT_DIR/$project_name'
  checkpoint_name: null # filename or use last from checkpoint_log.yaml, also can be set 'checkpoint_best.pt'

eval:
  eval_every: 250
  eval_iters: 1600 # 200*8 workers
  save_checkpoint: true
  checkoint_after: 0 # starts saving checkpoint after these steps
  checkpoint_every_hr: 200 # take checkpoint every this hours
  checkpoint_keep_best: false # keep only the best checkpoint, otherwise keep all with  _{step}.pt
