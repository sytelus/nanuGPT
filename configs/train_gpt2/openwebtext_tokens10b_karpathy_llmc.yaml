__include__: ['base_config.yaml']

# Config from Karpathy's llm.c:
# https://github.com/karpathy/llm.c/discussions/481
# https://github.com/karpathy/llm.c/blob/7ecd8906afe6ed7a2b2cdb731c042f26d525b820/scripts/run_gpt2_124M.sh

general:
  project_name: 'gpt2_owt'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/val.bin'
    eval_batch_size: '_copy: /training/device_batch_size'

training:
  device_batch_size: 32
  max_steps: 18865 # keller uses 9536, for 10.6B tokens, use 20345
  global_batch_size: 512

optimizer:
  module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
  module_kwargs:
    zero_stage: 1 # 0: off, 1: shard optimizer

scheduler:
  module: 'nanugpt.schedulers.linear.get_scheduler'
  module_kwargs:
    warmup_iters: 700
    end_factor: 1.0E-3

eval:
  eval_every: 250
  eval_iters: 1600 # 200*8 workers
  save_checkpoint: true
  checkoint_after: 0 # starts saving checkpoint after these steps
  checkpoint_every_hr: 200 # take checkpoint every this hours
  checkpoint_keep_best: false # keep only the best checkpoint, otherwise keep all with  _{step}.pt
