__include__: ['base_config.yaml']

# config for training GPT2 on OpenWebText with steps to match 10B tokens and
# eval settings same as Karpathy's llm.c with no checkpointing

general:
  project_name: 'gpt2_owt'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/val.bin'
    eval_batch_size: 32 # same as llm.c

# model:
#   module: 'nanugpt.models.nanogpt_keller.get_model'
#   module_kwargs:
#     n_layer: 12
#     n_embd: 768
#     n_head: 12
#     context_length: 1024

# scaler:
#   module: 'nanugpt.scalers.keller_scaler.get_scaler'
#   module_kwargs: {}

training:
  device_batch_size: 60 # default is 12 to match original nanogpt, should be 32 to match llm.c, can fit 60 on 192GB
  # some runs uses 23148 with 480 global batch size yielding 11.3777
  # for 480 global batch size, use 21701 to match 10.6665B tokens
  # for 512 global batch size, use 20345 to match 10.6666B tokens
  max_steps: 21701
  global_batch_size: 480 # original nanogpt 480, llm.c 512

# # keller AdamW setting
# optimizer:
#   module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
#   module_kwargs:
#     learning_rate: 18.0E-4 # default 6.0E-4, GH200 12.0E-4
#     zero_stage: 0 # 0: off, 1: shard optimizer

# # llm.c linear
# scheduler:
#   module: 'nanugpt.schedulers.linear.get_scheduler'
#   module_kwargs:
#     warmup_iters: 700
#     end_factor: 1.0E-3

# # keller WSD
# scheduler:
#   module: 'nanugpt.schedulers.constant.get_scheduler'
#   module_kwargs:
#     warmup_iters: 256
#     cooldown_iters: 2048
#     const_lr: 18.0E-4

eval:
  eval_every: 250
  eval_iters: 1600 # 200*8 workers
  save_checkpoint: false
  checkoint_after: 0 # starts saving checkpoint after these steps
  checkpoint_every_hr: 200 # take checkpoint every this hours
  checkpoint_keep_best: false # keep only the best checkpoint, otherwise keep all with  _{step}.pt
